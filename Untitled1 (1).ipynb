{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "ctrain = pd.read_csv(\"classification_train(1).csv\") \n",
    "ctest= pd.read_csv(\"classification_test(1).csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_split(data, num_folds):\n",
    "    fold_size = int(len(data) / num_folds)\n",
    "    data_perm = np.random.permutation(data)\n",
    "    folds = []\n",
    "    for k in range(num_folds):\n",
    "        folds.append(data_perm[k*fold_size:(k+1)*fold_size, :])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "#folds = cross_val_split(ctrain, 5)\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy(y, sample_weights=None):\n",
    "    if sample_weights is None:\n",
    "        sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "  \n",
    "    ce = 0\n",
    "    num = y.shape[0]  # number of labels\n",
    "    label_counts = {}  # caculate different labels in yï¼Œand store in label_counts\n",
    "    for i in range(num):\n",
    "        if y[i] not in label_counts.keys():\n",
    "            label_counts[y[i]] = 0\n",
    "        label_counts[y[i]] += sample_weights[i]\n",
    "  \n",
    "    for key in label_counts:\n",
    "        prob = float(label_counts[key]) / float(np.sum(sample_weights))\n",
    "        ce += prob * np.log(1- prob)  ## <-- SOLUTION\n",
    "\n",
    "    return ce\n",
    "\n",
    "\n",
    "def cross_1(X, y, i, sample_weights):\n",
    "    if sample_weights is None:\n",
    "        sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "  \n",
    "    new_impurity = 0\n",
    "    old_cost = cross_entropy(y, sample_weights)\n",
    "  \n",
    "    unique_vals = np.unique(X[:,i])\n",
    "    new_cost = 0.0\n",
    "  #split the values of i-th feature and calculate the cost \n",
    "    for value in unique_vals:\n",
    "        sub_X, sub_y, sub_sample_weights = split_dataset(X, y, i, value, sample_weights) ## <-- SOLUTION\n",
    "        prob = np.sum(sub_sample_weights) / float(np.sum(sample_weights))\n",
    "        new_cost += np.log(prob) * cross_entropy(sub_y, sub_sample_weights) ## <-- SOLUTION\n",
    "  \n",
    "    new_impurity = old_cost - new_cost # information gain\n",
    "\n",
    "    return new_impurity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(X, y, i, value, sample_weights):\n",
    "    ret = []\n",
    "    featVec = X[:,i]\n",
    "    X = X[:,[j for j in range(X.shape[1]) if j!=i]]\n",
    "  \n",
    "    for j in range(len(featVec)):\n",
    "        if featVec[j]==value:\n",
    "            ret.append(j)\n",
    "    sub_X = X[ret,:]\n",
    "    sub_y = y[ret]\n",
    "    sub_sample_weights = sample_weights[ret]\n",
    "    return sub_X, sub_y, sub_sample_weights\n",
    "\n",
    "\n",
    "def choose_best_feature(X, y, n_features,sample_weights):\n",
    "    \n",
    "    sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "    best_feature_idx = 0  \n",
    "    best_gain_cost = 0.0\n",
    "    if n_features ==10:\n",
    "        n_features = X.shape[1]\n",
    "        for i in range(n_features):\n",
    "            info_gain_cost = cross_1(X, y, i, sample_weights)           \n",
    "            if info_gain_cost > best_gain_cost:\n",
    "                best_gain_cost = info_gain_cost\n",
    "                best_feature_idx = i  \n",
    "    else: \n",
    "        for i in np.linspace(0,n_features-1,n_features):\n",
    "            #X= X[:,:(n_features)]\n",
    "            info_gain_cost = cross_1(X, y, int(i), sample_weights)          \n",
    "            if info_gain_cost > best_gain_cost:\n",
    "                best_gain_cost = info_gain_cost\n",
    "                best_feature_idx = int(i)                \n",
    "\n",
    "    return best_feature_idx\n",
    "\n",
    "\n",
    "def majority_vote(y, sample_weights=None):\n",
    "\n",
    "    if sample_weights is None:\n",
    "        sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "    majority_label = y[0]\n",
    "    dict_num = {}\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] not in dict_num.keys():\n",
    "            dict_num[y[i]] = sample_weights[i]\n",
    "        else:\n",
    "            dict_num[y[i]] += sample_weights[i]\n",
    "    majority_label = max(dict_num, key=dict_num.get)\n",
    "  # end answer\n",
    "    return majority_label\n",
    "\n",
    "\n",
    "def build_tree(X, y, feature_names, depth,max_depth,n_features, sample_weights):\n",
    "    mytree = dict()\n",
    "\n",
    "  # include a clause for the cases where (i) no feature, \n",
    "    #(ii) all lables are the same, (iii) depth exceed, or (iv) X is too small\n",
    "    if len(feature_names)==0 or len(np.unique(y))==1 or depth>=max_depth or len(X)<=2: \n",
    "        return majority_vote(y, sample_weights)\n",
    "  \n",
    "    else:\n",
    "        best_feature_idx = choose_best_feature(X, y, n_features, sample_weights)\n",
    "        best_feature_name = feature_names[best_feature_idx]\n",
    "        feature_names = feature_names[:]\n",
    "        feature_names.remove(best_feature_name)\n",
    "    \n",
    "        mytree = {best_feature_name:{}}\n",
    "        unique_vals = np.unique(X[:, best_feature_idx])\n",
    "        for value in unique_vals:\n",
    "            sub_X, sub_y, sub_sample_weights = split_dataset(X, y, best_feature_idx, value, sample_weights)  \n",
    "            mytree[best_feature_name][value] = build_tree(sub_X, sub_y, \n",
    "                                                          feature_names, depth+1, max_depth, n_features, sub_sample_weights) \n",
    "\n",
    "        return mytree\n",
    "\n",
    "    \n",
    "    \n",
    "def train(X, y, max_depth, n_features, sample_weights=None):\n",
    "    feature_names= ['a','b','c','d','e','f','g','h','i','j','k'] #create feature names\n",
    "    if sample_weights is None:\n",
    "      # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "    X = np.array(X)\n",
    "    #y = np.array(y)\n",
    "    depth=1\n",
    "    tree = build_tree(X, y, feature_names, depth,max_depth, n_features, sample_weights)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "def classify(tree, x):\n",
    "    feature_names = list(tree.keys())[0] # first element\n",
    "    second_dict = tree[feature_names]            \n",
    "    key = x.loc[feature_names]\n",
    "    if key not in second_dict:\n",
    "        key = np.random.choice(list(second_dict.keys()))\n",
    "    value_of_key = second_dict[key]\n",
    "    if isinstance(value_of_key, dict):\n",
    "        label = classify(value_of_key, x)\n",
    "    else:\n",
    "        label=value_of_key\n",
    "    return label\n",
    "\n",
    "def predict(X, tree):\n",
    "    if len(X.shape)==1:\n",
    "        return classify(tree, X)\n",
    "    else:\n",
    "        results=[]\n",
    "        N,D= X.shape\n",
    "        feature_names= ['a','b','c','d','e','f','g','h','i','j','k']\n",
    "        feature_names= feature_names[0:D]\n",
    "        X_df= pd.DataFrame(X, columns= feature_names)\n",
    "        for i in range(X.shape[0]):\n",
    "            results.append(classify(tree, X_df.iloc[i, :]))\n",
    "        return np.array(results)\n",
    "    \n",
    "    \n",
    "#Create a random subsample from the dataset with replacement\n",
    "def subsample(data):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(data))\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(data))\n",
    "\t\tsample.append(data[index])\n",
    "\treturn sample\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "def score(X_test, y_test, tree):\n",
    "    y_pred = predict(X_test, tree) ## <-- SOLUTION\n",
    "    y_pred= np.array(y_pred)\n",
    "    return np.float(sum(y_pred==y_test)) / float(len(y_test))\n",
    "\n",
    "#Random forest algorithm\n",
    "def random_forest(trainset, testset, max_depth, n_trees, n_features, sample_weights=None):\n",
    "    accuracies= list()\n",
    "    for i in range(n_trees):\n",
    "        #Bootstrapping\n",
    "        sample = subsample(trainset)\n",
    "        sample= np.array(sample)  \n",
    "        #Ensemble of models:\n",
    "        tree = train(sample[:,:-1], sample[:,-1], max_depth, n_features, sample_weights=None)\n",
    "        #Aggregating:\n",
    "        y_pred=predict(sample[:,:-1], tree)\n",
    "        accuracy = accuracy_metric(testset, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    index= np.argmax(accuracies)\n",
    "    maxaccuracy= accuracies[index]\n",
    "    return maxaccuracy\n",
    "\n",
    "\n",
    "## EDIT THIS FUNCTION\n",
    "def cross_val_evaluate_rf(data, num_folds, n_features, max_depth, n_trees):\n",
    "  \n",
    "  folds = cross_val_split(data, num_folds)\n",
    "\n",
    "  train_scores = []\n",
    "  val_scores = []\n",
    "\n",
    "  for i in range(len(folds)):\n",
    "    print('Fold', i+1)\n",
    "    # define the training set\n",
    "    train_set = np.delete(np.asarray(folds).reshape(len(folds), folds[0].shape[0], folds[0].shape[1]), i, axis=0)\n",
    "    train_folds = train_set.reshape(len(train_set)*train_set[0].shape[0], train_set[0].shape[1])\n",
    "    X_train1 = train_folds[:,:-1]\n",
    "    #X_train1 = pd.DataFrame(X_train1)\n",
    "    y_train1 = train_folds[:, -1]\n",
    "    #y_train1 = pd.DataFrame(y_train1)\n",
    "    #y_train1 = y_train1.T.squeeze()\n",
    "    \n",
    "    # define the validation set\n",
    "    val_fold = folds[i]\n",
    "    X_val = val_fold[:,:-1]\n",
    "    #X_val = pd.DataFrame(X_val)\n",
    "    y_val = val_fold[:, -1]\n",
    "    #y_val = pd.DataFrame(y_val)\n",
    "    #y_val = y_val.T.squeeze()\n",
    "\n",
    "    # train the model\n",
    "    #w_train = bagging_train_forest(X_train1, y_train1, n_features, max_depth, n_trees)\n",
    "    w_val = train(X_val, y_val, n_features, max_depth, n_trees)\n",
    "    #W_train = predict(X_train1,w_train)\n",
    "    W_val = predict(X_val, w_val)\n",
    "    ##W = sgd(X_train1, y_train1, max_iterations=1025, stop_criterion=0.01, learning_rate=1e-3, regul_strength=1e3)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # evaluate\n",
    "    #train_score = scoredt(W, y_train1)\n",
    "    val_score = (W_val, y_val)\n",
    "    #print(\"Accuracy on train set #{}: {}\".format(i+1, train_score))\n",
    "    print(\"Accuracy on validation set #{}: {}\".format(i+1, val_score))\n",
    "\n",
    "    #train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "  return val_scores #train_scores, val_scores\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[66.91823899 64.40251572]\n  [56.98113208 56.35220126]]\n\n [[67.67295597 67.42138365]\n  [61.25786164 63.39622642]]]\n"
     ]
    }
   ],
   "source": [
    "print(meanscores) # first matrix accuracy \n",
    "# second matrix= 2nd value of entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}